# Параметры запуска
- `--data_path` путь к папкам с датасетом
- `--load_all_data` если указано, то грузится весь датасет (прод), если не указан, то грузится два файла (отладка)
- `--print_date_samples` если указано, то печаютаются примеры датасетов
- `--test_start` дата начала тестовой выборки
- `--task_type` тип задачи: эмбеддинги для холодного старта (метрика LRAP) или обычная (метрика hit_rate_top_k)
- `--model_type` тип модели
- `--device` torch device, используется не во всех моделях
- `--history_file` в этот файл будет дописываться лог запуска в формате json

Для нейронки:
- `--max_epoch` число эпох
- `--optim_weight_decay` weight_decay для оптимизатора   
- `--optim_lr` lr для оптимизатора
- `--hidden_size` размер эмбеддинга (он же число факторов в ALS модели)

- `--user_one_for_all_size` размер эмбеддинга, общего для всех юзеров. 0, если не используется
- `--user_learn_embedding_size` размер эмбеддинга, обучаемого для каждого юзера. 0, если не используется
- `--user_fixed_vector_size` размер эмбеддинга, заданного извне для каждого юзера. 0, если не используется

- `--item_one_for_all_size` размер эмбеддинга, общего для всех item. 0, если не используется
- `--item_learn_embedding_size` размер эмбеддинга, обучаемого для каждого item. 0, если не используется
- `--item_fixed_vector_size` размер эмбеддинга, заданного извне для каждого item. 0, если не используется

- `--loss` тип loss
- `--loss_margin` margin для ranking loss
- `--nn_activation` тип активации
- 
- `--train_sample_rate` доля данных из трейна, которую надо использовать в каждой эпохе. Данные семплируются так, чтобы число 0 и 1 было одинаковым
- `--train_batch_size` размер батча в train
- `--valid_batch_size` размер батча на валидации. Используется во всех типах моделей
- `--train_num_workers` число воркеров для train data loader
- `--valid_num_workers` число воркеров для valid data loader
 
Для модели по популярности:
- `--exclude_seen_items` убрать из предсказания те айтемы, которые юзер уже видел. Работает не вл всех моделях
- `--use_mean_relevance` если задано, то популярность дял юзера считается как среднее меджу его нулями и 1. Показывает вероятнсть клика по айтему для конкретного юзера
- `--use_item_freq_predict` если задано, то использовать популярность как среднее по relevance для item, то есть популярность - это вероятность клика по показанному item. Иначе используется число кликов по item.
- `--use_low_freq_train_items` если задано, то популярность считается только на налопросматриваемых айтемах (дл рекомендации это плохо работает)

Для ALS:
- `--pop_norm` нормировать по популярности

Для задачи с метрикой hit_rate_top_k
- `--hit_rate_k` число K

# Что работает и что нет (`task_type=cold_start`)
## О задаче
Модель учится на train (сентябрь) и проверяется на valid (один день октября).
Предлагаем модели отранжировать item, которые были показаны пользователю на валидации.
Оцениваем качество ранжирования по метрике label_ranking_average_precision.
Она позволяет оценить качество ранжирования,  и работает если пользователь смотрел несколько item из предложенных.

При оценке валидационная выборка разбивается на две части:
 - `cold` - клиенты, которых не было в обучающей выборке
 - `warm` - клиенты, которые были в обучающей выборке
 
Также оценивается `all` - качество на всей выборке

Общее для всех моделей:
- `--exclude_seen_items` немного добавляет чуть меньше 1% к качеству. Перекрытие train и valid составляет 1%, так что особо нечего исключать.
 
## RandomModel
Модель, которая ранжирует items случайным образом дает слудующее качество:
 - all: 0.5483
 - cold: 0.5413
 - warm: 0.5548
 
Запуск этой модели через скрипт не реализован.

## PopularModel 
Модель, основанная на популярности item на лучших настройках дает следующее качество:
 - all: 0.6396
 - cold: 0.6431
 - warm: 0.6363

Популярность при этом считается как отношение просомтров item к общему числу его показов, то есть как вероятность клика по показанному item.
```
# how to run
python -m stories_recsys --data_path 'some_path' --load_all_data --exclude_seen_items --model_type popular --use_item_freq_predict
```

## ALSModel (базовый)
### ALS базовый
Если в качестве user_item матрицы подать `1` - клик, `-1` не клик, то качество получается на уровне случайного ранжирования.
 - all: 0.5528
 - cold: 0.5400
 - warm: 0.5647

```
# how to run
python -m stories_recsys --data_path 'some_path' --load_all_data --exclude_seen_items --model_type als
```

### ALS с популярностью
Если в качестве user_item матрицы взять значения, центрированные по популярности item, то качество достигает модели по популярности.
 - all: 0.6382
 - cold: 0.6431
 - warm: 0.6336

Алгоритм:
  1. Считаем популярность всех item
  2. Вычитаем из 0 (не лайк) или (1 - лайк) популярностть item. Получаем насколько этот юзер больше склонен к этому item, чем среднее по выборке.
    Например если популярность была 0.8, а юзер смотрел - то в user_item идет 0.2 - небольшая склонность к просмотру.
    А если популярность была 0.8, а юзер не смотрел - то в user_item идет -0.8 - большая склонность к не просмотру.
  3. Обучаем ALS
  4. Во время predict, прибавляем обратно популярность item.
  
```
# how to run
python -m stories_recsys --data_path 'some_path' --load_all_data --exclude_seen_items --model_type als --pop_norm
```

## Нейронная сеть
Нейронную сеть можно запускать в разных режимах, который имитируют предыдущие модели, или их комбинацию.

### Как ALS
Здесь мы учим для каждого пользователя свой вектор. Нужно заметить, что есть пользователи, которые смотрели мало item,
которых мы заменяем на обучаемый padding вектор `embedding[0]`.

Модель дает следующее качество:
 - all: 0.5969
 - cold: 0.6208
 - warm: 0.5747

Для тех пользователей, которых модель видела, качество получается на уровне ALS - на уровне рандома.
Для тех пользователей, котоорых модель не видела, учится общий вектор `embedding[0]`, который работает на уровне популярной модели,
поскольку учится на тех же цифрах, что и популярная модель, только на части выборки.

```
# how to run
python -m stories_recsys --data_path 'some_path' --load_all_data --exclude_seen_items --model_type nn \
    --user_learn_embedding_size 32 --item_learn_embedding_size 32 \
    --loss bce --nn_activation sigmoid --optim_weight_decay 0.0001 --optim_lr 0.001
```

### Как популярность
Можно выучить для всех пользователей один общий вектор. Тогда качество будет как в модели по поулярности как для cold, так и для warm
Качество:
 - all: 0.6387	
 - cold: 0.6416	
 - warm: 0.6360

```
# how to run
python -m stories_recsys --data_path 'some_path' --load_all_data --exclude_seen_items --model_type nn \
    --user_one_for_all_size 32 --item_learn_embedding_size 32 \
    --loss bce --nn_activation sigmoid --optim_weight_decay 0.0001 --optim_lr 0.001
```

### Если добавить готовые эмбеддинги
Готовые векторы позволят получить признаки мользователя, которые указываю на склонность к просомтру определенных item 
не только для тех пользователей, которые есть в тестовой выборке, но и для новых.

Поскольку ALS не выявил таких признаков в текущих пользователях, можно сказать, что и в готовых векторах таких признаков нет.

Качество остается на уровне популярной модели:
 - all: 0.6413
 - cold: 0.6467
 - warm: 0.6363

Скорее всего во время трансформации исходных векторов 256 в пространство 32 модель переносит векторы в одну небольшую область, 
под которую учит популярность. 

```
# how to run
python -m stories_recsys --data_path 'some_path' --load_all_data --exclude_seen_items --model_type nn \
    --user_fixed_vector_size 256 --item_learn_embedding_size 32 \
    --loss bce --nn_activation sigmoid --optim_weight_decay 0.0001 --optim_lr 0.001
```


# Выводы
Хорошо работает модель по популярности, которая не различает юзеров.

ALS не дает какого-то преимущества для тех пользоваталей, которых он видел (warm) по сравнению с теми, котрых не видел (cold).
Качество на ALS показывает, что в данных нет каких-то паттернов по совместному просмотру item, которые бы позволили 
объединить некоторых юзеров в группы. У пользователя нет таких свойств, которые бы позволили сказать какие item ему понравятся.

Добавление внешних эмбеддиногов не прибавляет качество, потому что нет признаков, которые можно было бы привязать к юзеру
и извлечь их из какого-либо вектора.
