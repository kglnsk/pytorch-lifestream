{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_AUGMENTS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "SAMPLING_STRATEGY = 'HardNegativePair'\n",
    "NEGATIVES_COUNT = 2#N_AUGMENTS + 1\n",
    "MARGING = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Normalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L2Normalization, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.div(torch.norm(input, dim=1).view(-1, 1))\n",
    "    \n",
    "    \n",
    "def outer_pairwise_distance(A, B=None):\n",
    "    \"\"\"\n",
    "        Compute pairwise_distance of Tensors\n",
    "            A (size(A) = n x d, where n - rows count, d - vector size) and\n",
    "            B (size(A) = m x d, where m - rows count, d - vector size)\n",
    "        return matrix C (size n x m), such as C_ij = distance(i-th row matrix A, j-th row matrix B)\n",
    "\n",
    "        if only one Tensor was given, computer pairwise distance to itself (B = A)\n",
    "    \"\"\"\n",
    "\n",
    "    if B is None: B = A\n",
    "\n",
    "    max_size = 2 ** 26\n",
    "    n = A.size(0)\n",
    "    m = B.size(0)\n",
    "    d = A.size(1)\n",
    "\n",
    "    if n * m * d <= max_size or m == 1:\n",
    "\n",
    "        return torch.pairwise_distance(\n",
    "            A[:, None].expand(n, m, d).reshape((-1, d)),\n",
    "            B.expand(n, m, d).reshape((-1, d))\n",
    "        ).reshape((n, m))\n",
    "\n",
    "    else:\n",
    "\n",
    "        batch_size = max(1, max_size // (n * d))\n",
    "        batch_results = []\n",
    "        for i in range((m - 1) // batch_size + 1):\n",
    "            id_left = i * batch_size\n",
    "            id_rigth = min((i + 1) * batch_size, m)\n",
    "            batch_results.append(outer_pairwise_distance(A, B[id_left:id_rigth]))\n",
    "\n",
    "        return torch.cat(batch_results, dim=1)\n",
    "    \n",
    "def outer_cosine_similarity(A, B=None):\n",
    "    \"\"\"\n",
    "        Compute cosine_similarity of Tensors\n",
    "            A (size(A) = n x d, where n - rows count, d - vector size) and\n",
    "            B (size(A) = m x d, where m - rows count, d - vector size)\n",
    "        return matrix C (size n x m), such as C_ij = cosine_similarity(i-th row matrix A, j-th row matrix B)\n",
    "\n",
    "        if only one Tensor was given, computer pairwise distance to itself (B = A)\n",
    "    \"\"\"\n",
    "\n",
    "    if B is None: B = A\n",
    "\n",
    "    max_size = 2 ** 32\n",
    "    n = A.size(0)\n",
    "    m = B.size(0)\n",
    "    d = A.size(1)\n",
    "\n",
    "    if n * m * d <= max_size or m == 1:\n",
    "\n",
    "        A_norm = torch.div(A.transpose(0, 1), A.norm(dim=1)).transpose(0, 1)\n",
    "        B_norm = torch.div(B.transpose(0, 1), B.norm(dim=1)).transpose(0, 1)\n",
    "        return torch.mm(A_norm, B_norm.transpose(0, 1))\n",
    "\n",
    "    else:\n",
    "\n",
    "        batch_size = max(1, max_size // (n * d))\n",
    "        batch_results = []\n",
    "        for i in range((m - 1) // batch_size + 1):\n",
    "            id_left = i * batch_size\n",
    "            id_rigth = min((i + 1) * batch_size, m)\n",
    "            batch_results.append(outer_cosine_similarity(A, B[id_left:id_rigth]))\n",
    "\n",
    "        return torch.cat(batch_results, dim=1)\n",
    "    \n",
    "def metric_Recall_top_K(X, y, K, metric='cosine'):\n",
    "    \"\"\"\n",
    "        calculate metric R@K\n",
    "        X - tensor with size n x d, where n - number of examples, d - size of embedding vectors\n",
    "        y - true labels\n",
    "        N - count of closest examples, which we consider for recall calcualtion\n",
    "        metric: 'cosine' / 'euclidean'.\n",
    "            !!! 'euclidean' - to slow for datasets bigger than 100K rows\n",
    "    \"\"\"\n",
    "    res = []\n",
    "\n",
    "    n = X.size(0)\n",
    "    d = X.size(1)\n",
    "    max_size = 2 ** 32\n",
    "    batch_size = max(1, max_size // (n*d))\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(1 + (len(X) - 1) // batch_size):\n",
    "\n",
    "            id_left = i*batch_size\n",
    "            id_right = min((i+1)*batch_size, len(y))\n",
    "            y_batch = y[id_left:id_right]\n",
    "\n",
    "            if metric == 'cosine':\n",
    "                pdist = -1 * outer_cosine_similarity(X, X[id_left:id_right])\n",
    "            elif metric == 'euclidean':\n",
    "                pdist = outer_pairwise_distance(X, X[id_left:id_right])\n",
    "            else:\n",
    "                raise AttributeError(f'wrong metric \"{metric}\"')\n",
    "\n",
    "            values, indices = pdist.topk(K + 1, 0, largest=False)\n",
    "\n",
    "            y_rep = y_batch.repeat(K, 1)\n",
    "            res.append((y[indices[1:]] == y_rep).sum().item())\n",
    "\n",
    "    return np.sum(res) / len(y) / K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairSelector:\n",
    "    \"\"\"\n",
    "    Implementation should return indices of positive pairs and negative pairs that will be passed to compute\n",
    "    Contrastive Loss\n",
    "    return positive_pairs, negative_pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_pairs(self, embeddings, labels):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class HardNegativePairSelector(PairSelector):\n",
    "    \"\"\"\n",
    "    Generates all possible possitive pairs given labels and\n",
    "         neg_count hardest negative example for each example\n",
    "    \"\"\"\n",
    "    def __init__(self, neg_count = 1):\n",
    "        super(HardNegativePairSelector, self).__init__()\n",
    "        self.neg_count = neg_count\n",
    "\n",
    "    def get_pairs(self, embeddings, labels):\n",
    "        \n",
    "        # construct matrix x, such as x_ij == 0 <==> labels[i] == labels[j]\n",
    "        n = labels.size(0)\n",
    "        x = labels.expand(n,n) - labels.expand(n,n).t()        \n",
    "            \n",
    "        # positive pairs\n",
    "        positive_pairs = torch.triu((x == 0).int(), diagonal = 1).nonzero()\n",
    "        \n",
    "        # hard negative minning\n",
    "        mat_distances = outer_pairwise_distance(embeddings.detach()) # pairwise_distance\n",
    "        \n",
    "        upper_bound = int((2*n) ** 0.5) + 1\n",
    "        mat_distances = ((upper_bound - mat_distances) * (x != 0).type(mat_distances.dtype)) # filter: get only negative pairs\n",
    "        \n",
    "        values, indices = mat_distances.topk(k = self.neg_count, dim = 0, largest = True)\n",
    "        negative_pairs = torch.stack([\n",
    "            torch.arange(0,n, dtype = indices.dtype, device = indices.device).repeat(self.neg_count),\n",
    "            torch.cat(indices.unbind(dim = 0))\n",
    "        ]).t()\n",
    "\n",
    "        return positive_pairs, negative_pairs\n",
    "        \n",
    "def get_sampling_strategy(params='HardNegativePair'):\n",
    "    if params == 'HardNegativePair':\n",
    "        kwargs = {\n",
    "            'neg_count' : NEGATIVES_COUNT,\n",
    "        }\n",
    "        kwargs = {k:v for k,v in kwargs.items() if v is not None}\n",
    "        sampling_strategy = HardNegativePairSelector(**kwargs)\n",
    "        return sampling_strategy\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS/MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_dataset = torchvision.datasets.MNIST('/mnt/data/molchanov/datasets/mnist', \n",
    "                                           train=True, \n",
    "                                           transform=None, \n",
    "                                           target_transform=None, \n",
    "                                           download=True)\n",
    "\n",
    "mnist_test_dataset = torchvision.datasets.MNIST('/mnt/data/molchanov/datasets/mnist', \n",
    "                                           train=False, \n",
    "                                           transform=None, \n",
    "                                           target_transform=None, \n",
    "                                           download=True)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# very slow\n",
    "def mnist_augmentation(p=1):\n",
    "    return Compose([\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=1),\n",
    "        OneOf([\n",
    "            MotionBlur(p=.33),\n",
    "            MedianBlur(blur_limit=3, p=0.33),\n",
    "            Blur(blur_limit=3, p=0.33),\n",
    "        ], p=1),\n",
    "        OneOf([\n",
    "            OpticalDistortion(p=0.33),\n",
    "            GridDistortion(p=0.33),\n",
    "            IAAPiecewiseAffine(p=0.33)\n",
    "        ], p=1),\n",
    "        OneOf([\n",
    "            IAAAdditiveGaussianNoise(),\n",
    "            GaussNoise(),\n",
    "        ], p=1),\n",
    "    ], p=p)\n",
    "\n",
    "\n",
    "def mnist_torch_augmentation(p=1):\n",
    "    return torchvision.transforms.Compose([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomAffine(degrees=7, \n",
    "                                translate=(0.1, 0.1), \n",
    "                                scale=(0.9, 0.9), \n",
    "                                shear=None, \n",
    "                                resample=False, \n",
    "                                fillcolor=0),\n",
    "            transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3)\n",
    "        ]),\n",
    "        transforms.ColorJitter(brightness=0.7, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        #transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "    ])\n",
    "plt.imshow(mnist_test_dataset[0][0])\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "class MnistClassificationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.norm = L2Normalization()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) == 4:\n",
    "            x = x[:, -1, :, :].unsqueeze(1) # get augmented image\n",
    "        else:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.norm(x)\n",
    "        return output\n",
    "    \n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "class MnistMetricLearningNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, x.size(-2), x.size(-1)) # b, augs, x, y -> b*augs, 1, x, y\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train_dataset = torchvision.datasets.CIFAR10('/mnt/data/molchanov/datasets/cifar10', \n",
    "                                               train=True, \n",
    "                                               transform=None, \n",
    "                                               target_transform=None, \n",
    "                                               download=True)\n",
    "\n",
    "cifar10_test_dataset = torchvision.datasets.CIFAR10('/mnt/data/molchanov/datasets/cifar10', \n",
    "                                               train=False, \n",
    "                                               transform=None, \n",
    "                                               target_transform=None, \n",
    "                                               download=True)\n",
    "\n",
    "plt.imshow(cifar10_test_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_train_dataset = torchvision.datasets.CIFAR100('/mnt/data/molchanov/datasets/cifar100', \n",
    "                                               train=True, \n",
    "                                               transform=None, \n",
    "                                               target_transform=None, \n",
    "                                               download=True)\n",
    "\n",
    "cifar100_test_dataset = torchvision.datasets.CIFAR100('/mnt/data/molchanov/datasets/cifar100', \n",
    "                                               train=False, \n",
    "                                               transform=None, \n",
    "                                               target_transform=None, \n",
    "                                               download=True)\n",
    "\n",
    "plt.imshow(cifar100_test_dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetrLearnDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, augmenter, n_augments=10):\n",
    "        self.data = dataset\n",
    "        self.aug = augmenter\n",
    "        self.n_augments = n_augments\n",
    "        \n",
    "    def draw(self, idx):\n",
    "        imgs, lbl = self[idx]\n",
    "        print(imgs.shape, lbl)\n",
    "        fig = plt.figure()\n",
    "        rows, columns = 1, imgs.shape[0]\n",
    "        for i in range(imgs.shape[0]):\n",
    "            fig.add_subplot(rows, columns, i+1)\n",
    "            plt.imshow(imgs[i])\n",
    "        plt.show()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    '''\n",
    "    def __getitem__(self, idx):\n",
    "        img, lbl = self.data[idx]\n",
    "        imgs = [np.array(img)] + [self.aug(image=np.array(img))['image'] for i in range(self.n_augments)]\n",
    "        b_img = np.stack(imgs, axis=0)\n",
    "        return b_img, lbl\n",
    "    '''\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, lbl = self.data[idx]\n",
    "        imgs = [transforms.ToTensor()(img)] + [self.aug(img) for i in range(self.n_augments)]\n",
    "        b_img = torch.stack(imgs).squeeze()\n",
    "        return b_img, lbl\n",
    "        \n",
    "    \n",
    "def get_mnist_train_loader(batch_size, n_augments=4):\n",
    "    data_train = MetrLearnDataset(dataset=mnist_train_dataset, \n",
    "                            augmenter=mnist_torch_augmentation(p=1), \n",
    "                            n_augments=n_augments)\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(data_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=16)\n",
    "    return train_data_loader\n",
    "    \n",
    "def get_mnist_test_loader(batch_size, n_augments=4):\n",
    "    data_test = MetrLearnDataset(dataset=mnist_test_dataset, \n",
    "                            augmenter=mnist_torch_augmentation(p=1), \n",
    "                            n_augments=n_augments)\n",
    "        \n",
    "    test_data_loader = torch.utils.data.DataLoader(data_test,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=5)\n",
    "    return test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = MetrLearnDataset(dataset=mnist_test_dataset, \n",
    "                        augmenter=mnist_torch_augmentation(p=1), \n",
    "                        n_augments=N_AUGMENTS)\n",
    "data.draw(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "    \n",
    "    \"Signature verification using a siamese time delay neural network\", NIPS 1993\n",
    "    https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, pair_selector):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pair_selector = pair_selector\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "        \n",
    "        positive_pairs, negative_pairs = self.pair_selector.get_pairs(embeddings, target)\n",
    "        positive_loss = F.pairwise_distance(embeddings[positive_pairs[:, 0]], embeddings[positive_pairs[:, 1]]).pow(2)\n",
    "        \n",
    "        negative_loss = F.relu(\n",
    "            self.margin - F.pairwise_distance(embeddings[negative_pairs[:, 0]], embeddings[negative_pairs[:, 1]])\n",
    "        ).pow(2)\n",
    "        loss = torch.cat([positive_loss, negative_loss], dim=0)\n",
    "        \n",
    "        return loss.sum(), len(positive_pairs) + len(negative_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaunchInfo:\n",
    "    \n",
    "    def __init__(self, model, loss, optimizer, scheduler, train_loader, test_loader, epochs, device, mode):\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.epochs = epochs\n",
    "        self.mode = mode\n",
    "        \n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            self.scheduler = scheduler\n",
    "        else:\n",
    "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,\n",
    "                                                             step_size=1,\n",
    "                                                             gamma=GAMMA)\n",
    "            \n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            \n",
    "        self.model.to(device)\n",
    "        self.loss.to(device)\n",
    "        \n",
    "# -------------------------- predefind configs --------------------------------\n",
    "        \n",
    "mnist_classification_lunch_info = LaunchInfo(model=MnistClassificationNet(), \n",
    "                                             loss=torch.nn.NLLLoss(), \n",
    "                                             optimizer=None, \n",
    "                                             scheduler=None, \n",
    "                                             train_loader=get_mnist_train_loader(batch_size=BATCH_SIZE, \n",
    "                                                                                 n_augments=N_AUGMENTS), \n",
    "                                             test_loader=get_mnist_test_loader(batch_size=BATCH_SIZE, \n",
    "                                                                               n_augments=N_AUGMENTS), \n",
    "                                             epochs=EPOCHS, \n",
    "                                             device='cuda',\n",
    "                                             mode='classification')\n",
    "    \n",
    "mnist_metriclearning_lunch_info = LaunchInfo(model=MnistMetricLearningNet(), \n",
    "                                             loss=ContrastiveLoss(margin=MARGING, \n",
    "                                                                  pair_selector=get_sampling_strategy(SAMPLING_STRATEGY)), \n",
    "                                             optimizer=None, \n",
    "                                             scheduler=None, \n",
    "                                             train_loader=get_mnist_train_loader(batch_size=BATCH_SIZE, \n",
    "                                                                                 n_augments=N_AUGMENTS), \n",
    "                                             test_loader=get_mnist_test_loader(batch_size=BATCH_SIZE, \n",
    "                                                                               n_augments=N_AUGMENTS), \n",
    "                                             epochs=EPOCHS, \n",
    "                                             device='cuda',\n",
    "                                             mode='metric_learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    \n",
    "    def __init__(self, launch_info):\n",
    "        self.info = launch_info\n",
    "        \n",
    "        self.model = self.info.model\n",
    "        self.loss = self.info.loss\n",
    "        self.optimizer = self.info.optimizer\n",
    "        self.scheduler = self.info.scheduler\n",
    "        self.epochs = self.info.epochs\n",
    "        self.train_loader = self.info.train_loader\n",
    "        self.test_loader = self.info.test_loader\n",
    "        self.device = self.info.device\n",
    "        self.mode = self.info.mode\n",
    "        \n",
    "        \n",
    "    def train_epoch(self, step):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        with tqdm.notebook.tqdm(total=len(self.train_loader)) as steps:\n",
    "            for itr, data in enumerate(self.train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                device_data = self.data_to_device(data)\n",
    "                labels = device_data[1]\n",
    "                \n",
    "                out = self.model(device_data[0])\n",
    "                if self.mode == 'metric_learning':\n",
    "                    #labels = torch.arange(int(out.size(0)/(N_AUGMENTS + 1))).view(1, -1).repeat(N_AUGMENTS+1, 1).transpose(0, 1).flatten().to(self.device)\n",
    "                    labels = labels.view(1, -1).repeat(N_AUGMENTS+1, 1).transpose(0, 1).flatten().to(self.device)\n",
    "                \n",
    "                loss = self.loss(out, labels)\n",
    "                if self.mode == 'metric_learning':\n",
    "                    loss, pos_neg_len = loss[0]/loss[1], loss[1]\n",
    "                losses.append(loss.item())\n",
    "                loss_val = self.running_average(losses)\n",
    "                loss.backward()\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                \n",
    "                steps.set_description(f\"train: epoch {step}, step {itr}/{len(self.train_loader)}\")\n",
    "                if self.mode == 'classification':\n",
    "                    steps.set_postfix({\"loss\": '{:.5E}'.format(loss_val)})\n",
    "                elif self.mode == 'metric_learning':\n",
    "                    steps.set_postfix({\"loss\": '{:.5E}'.format(loss_val),\n",
    "                                       \"pos_neg_len\": pos_neg_len})\n",
    "                steps.update()\n",
    "        \n",
    "    def test_epoch(self, step):\n",
    "        self.model.eval()\n",
    "        losses, total, corrects = [], 0, 0\n",
    "        total_recall = 0\n",
    "        with torch.no_grad():\n",
    "            with tqdm.notebook.tqdm(total=len(self.test_loader)) as steps:\n",
    "                for itr, data in enumerate(self.test_loader):\n",
    "                    device_data = self.data_to_device(data)\n",
    "                    labels = device_data[1]\n",
    "                    \n",
    "                    out = self.model(device_data[0])\n",
    "                    if self.mode == 'metric_learning':\n",
    "                        #labels = torch.arange(int(out.size(0)/(N_AUGMENTS + 1))).view(1, -1).repeat(N_AUGMENTS+1, 1).transpose(0, 1).flatten().to(self.device)\n",
    "                        labels = labels.view(1, -1).repeat(N_AUGMENTS+1, 1).transpose(0, 1).flatten().to(self.device)\n",
    "                \n",
    "                    loss = self.loss(out, labels)\n",
    "                    if self.mode == 'metric_learning':\n",
    "                        loss, pos_neg_len = loss[0], loss[1]\n",
    "                    losses.append(loss.item())\n",
    "                    loss_val = self.running_average(losses)\n",
    "                    \n",
    "                    if self.mode == 'classification':\n",
    "                        pred = F.softmax(out, dim=-1).argmax(dim=-1)\n",
    "                        corrects += pred.eq(device_data[1].view_as(pred)).sum().item()\n",
    "                        total += pred.size(0)\n",
    "                        accuracy = corrects/total\n",
    "                    elif self.mode == 'metric_learning':\n",
    "                        batch_recall = metric_Recall_top_K(out, labels, N_AUGMENTS*int(BATCH_SIZE/10))\n",
    "                        total_recall += batch_recall\n",
    "                    \n",
    "                    steps.set_description(f\"test: epoch {step}, step {itr}/{len(self.train_loader)}\")\n",
    "                    if self.mode == 'classification':\n",
    "                        steps.set_postfix({\"loss\": '{:.5E}'.format(loss_val), \"accuracy\": accuracy})\n",
    "                    elif self.mode == 'metric_learning':\n",
    "                        steps.set_postfix({\"loss\": '{:.5E}'.format(loss_val),\n",
    "                                           \"batch_recall\": batch_recall,\n",
    "                                           \"total_recall\": total_recall/(itr+1)})\n",
    "                    steps.update()\n",
    "        \n",
    "    def fit(self):\n",
    "        for step in range(self.epochs):\n",
    "            self.train_epoch(step + 1)\n",
    "            self.test_epoch(step + 1)\n",
    "            self.scheduler.step()\n",
    "            \n",
    "    def data_to_device(self, data):\n",
    "        return data[0].to(self.device).float(), data[1].to(self.device)\n",
    "    \n",
    "    def running_average(self, x):\n",
    "        arr = np.array(x)[-1000:]\n",
    "        res = np.convolve(arr, np.ones((arr.shape[0],)))/arr.shape[0]\n",
    "        return res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mnist_classification_learner = Learner(launch_info=mnist_classification_lunch_info)\n",
    "mnist_classification_learner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist_metriclearning_learner = Learner(launch_info=mnist_metriclearning_lunch_info)\n",
    "mnist_metriclearning_learner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
