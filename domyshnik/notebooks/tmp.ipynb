{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/mnt/data/molchanov/dltranz')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function, Variable\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from domyshnik.models import *\n",
    "from domyshnik.data import *\n",
    "from domyshnik.constants import *\n",
    "from domyshnik.utils import *\n",
    "\n",
    "def draw(imgs):\n",
    "        if isinstance(imgs, list):\n",
    "            imgs = torch.stack(imgs)\n",
    "        fig = plt.figure()\n",
    "        rows, columns = 1, imgs.shape[0]\n",
    "        for i in range(imgs.shape[0]):\n",
    "            fig.add_subplot(rows, columns, i+1)\n",
    "            if imgs[i].size(0) == 3:\n",
    "                plt.imshow(imgs[i].transpose(0, 1).transpose(1, 2))\n",
    "            else:\n",
    "                plt.imshow(imgs[i])\n",
    "        plt.show()\n",
    "        \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "los = ClusterisationLoss(margin=0.5, \n",
    "                         input_dim=128, \n",
    "                         num_classes=10, \n",
    "                         device='cpu')\n",
    "\n",
    "t = torch.randn(120, 128)\n",
    "y, z = los(t)\n",
    "(y + z).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GambelSoftmax(nn.Module):\n",
    "    \n",
    "    def __init__(self, k, t):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.t = t\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    x - logit probs\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        u = torch.rand(self.k)\n",
    "        g = -1*torch.log(-1*torch.log(u))\n",
    "        return F.softmax((x + g)/t, dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, t = 10, 0.1\n",
    "g_softmax = GambelSoftmax(K, t)\n",
    "\n",
    "p = torch.log(torch.rand(K))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, g_softmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFunction(Function):\n",
    "        \n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        \n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight)\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        print(grad_output.size())\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        grad_input = grad_output.mm(weight.t())\n",
    "        grad_weight = grad_output.t().mm(input)\n",
    "        grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, w, b = torch.randn(10, 128), torch.randn(128, 64), torch.Tensor([-1.0])\n",
    "inp.requires_grad=True\n",
    "w.requires_grad=True\n",
    "b.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc = MyFunction.apply\n",
    "y = fnc(inp, w, b)\n",
    "z = y.sum()\n",
    "\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax grad\n",
    "batch_probs = torch.log(torch.rand(3, 10))\n",
    "batch_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = batch_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
