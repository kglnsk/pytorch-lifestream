{
  data_module: {
    type: map
    setup: {
      col_id: client_id
      dataset_files: {
        data_path: "data/train_trx.parquet"
      }
      split_by: files
      valid_size: 0.05
      valid_split_seed: 42
    }
    train: {
      min_seq_len: 30
      split_strategy: {
        split_strategy: "SampleSlices"
        split_count: 5
        cnt_min: 30
        cnt_max: 180
      }
      augmentations: [
        [DropoutTrx, {trx_dropout: 0.01}]
      ]
      num_workers: 16
      batch_size: 64
    }
    valid: {
      split_strategy: {
        split_strategy: SampleSlices
        split_count: 5
        cnt_min: 30
        cnt_max: 180
      }
      augmentations: []
      num_workers: 16
      batch_size: 256
    }
  }

  seed_everything: 42
  trainer: {
    gpus: 1
    auto_select_gpus: false

    max_epochs: 2

    checkpoint_callback: false
    deterministic: True
  }
  logger_name: agg_features_model

  params: {
    data_module_name: ColesDataModuleTrain
    pl_module_name: CoLESModule

    validation_metric_params: {
        K: 4
        metric: cosine
    }

    encoder_type: agg_features
    trx_encoder: {
      embeddings: {
        level_3: {in: 200},
        level_4: {in: 800},
        segment_id: {in: 120}
      }
      numeric_values: {
        trn_sum_from_iss: identity,
        netto: identity,
        regular_points_received: identity
      }
      was_logified: true
      log_scale_factor: 1
    },
    head_layers: [
        [BatchNorm1d, {num_features: "{seq_encoder.embedding_size}"}]
        [NormEncoder, {}],
    ]

    lr_scheduler: {
      step_size: 30,
      step_gamma: 0.9025
    },
    train: {
      sampling_strategy: HardNegativePair,
      neg_count: 5,
      loss: ContrastiveLoss,
      margin: 0.5,
      lr: 0.002,
      weight_decay: 0.0,
    }
  }

  model_path: "models/agg_features_model.p"

  include "dataset_inference_file.hocon"

  output: {
    path: data/agg_feat_embed
    format: pickle
  }
}
